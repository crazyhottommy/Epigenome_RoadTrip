### Random Forest

Lisa has suggested to use Random Forest to select the features for testing.

```{r}

library(dplyr)
library(rlang)
library(caret)

seg_df_sub<- read.table("data/epigenome_Roadmap/merged_segments/seg_df_chr1_sub.tsv", stringsAsFactors = F, sep = "\t", header =T)

meta_data<- read.table("data/epigenome_Roadmap/ids_with_rnaseq.tsv", stringsAsFactors = F, sep = "\t", header =T)

## need to figure out a better way to select the columns...
new_data<- seg_df_sub %>% distinct(E027, E028, E055, E056, E057, E058, E059, E061, E065, E075, E077, E079, E083, E084, E085, E092, E094, E095, E101, E102, E104, E105, E106, E109, E110, .keep_all = T)

x_data<- as.matrix(new_data[-c(1:3,ncol(new_data))])

rownames(x_data)<- paste(new_data$chr, new_data$start, new_data$end, sep = ":")

## transpose the matrix.
x_data<- t(x_data)

## random forest does not like matrix if it is not numeric. turn back to a dataframe
x_data<- data.frame(x_data)

resp_data<- merge(x = data.frame(Sample = rownames(x_data)), y = meta_data, by.x = "Sample", by.y = "X1", all.x = T, all.y = F)

resp_data$X2 = factor(resp_data$X2, levels = c("Epithelial", "Digestive", "Heart"))

sub_meta = subset(meta_data, X2 %in% c("Heart", "Digestive", "Epithelial"))
names(sub_meta)<- c("sample", "tissue", "rna_seq")

library(readr)
write_tsv(sub_meta[, c(1,2)] %>% mutate(sample = paste0(sample, ".bed.bgz")), "data/epigenome_Roadmap/sample_info.txt")

sample_ids = subset(meta_data, X2 %in% c("Heart", "Digestive", "Epithelial"))$X1

set.seed(1973)
cols2use = sample(1:ncol(x_data), 200)

data<- cbind(x_data, tissue = resp_data$X2)

## caret package https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/


## create dummy variables, caret assumes the predictors to be numeric. takes forever...
dummies <- dummyVars( tissue ~ .,  data = data)
head(predict(dummies, newdata = etitanic))


control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(x_data, resp_data$X2, sizes=c(1:8), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))


####
rf_mod = randomForest(x = x_data[,cols2use], y = resp_data$X2)
getTree(rf_mod)
# default ntree= 500,
getTree(rf_mod, labelVar =T, k=1)
getTree(rf_mod, labelVar =T, k=2)
```


### only use a subset of features to test.

I googled around and found `VSURF` package to do feature selections using randomForest.

```{r}
library(VSURF)

test_data<- x_data[,cols2use]

ti<- proc.time()
vsurf <- VSURF(x = x_data[,cols2use], y = resp_data$X2, ntree = 500, parallel =F)
proc.time() - ti

names(vsurf)
summary(vsurf)

plot(vsurf)
vsurf$varselect.thres
vsurf$varselect.interp
vsurf$varselect.pred

```

The running time of using all 440 k features can be very long. Lisa is testing on the server.

at the same time, even from those random 200 features, 7 are selected out as important ones. And I checked on IGV. some look
good to me to differentiate the subgroups.

```{r}
> vsurf$varselect.pred
[1] 160 155 153

colnames(test_data)[vsurf$varselect.pred]
[1] "chr1.94106400.94106600"   "chr1.200879400.200879800" "chr1.19945600.19945800"

```

### Think about using heatmap to visualize all features

    f1  f2  f3  f4 ....
s1  E01 E02 .....
s2  ....................
s3  ....................
.
.
.  
